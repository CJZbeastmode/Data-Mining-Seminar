\section{Implementation}
The problem of support vector machine is an optimization problem. The goal is, as mentioned, 
to find the optimal hyperplane that separates the data in a way that the margin is maximized.
Therefore, it is important to first define the hyperplane.

Since the hyperplane is a flat affine subspace of dimension $p$, we can easily define a hyperplane as \cite{R9}: 
\begin{equation}
    \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0
\end{equation}

We can then denote $X$ and $\beta$ as vectors \cite{Elements12}: 
\begin{equation}
    \{x \in \mathbb{R}^p: X^T \beta + \beta_0 = 0\}
\end{equation}

With hyperplane being defined, we are now able to observe the problem of separating the spaces of the hyperplane. 
In other words, we need to construct linear decision
boundaries that attempt to separate the data into two or more classes as precisely as possible. 
The main two approaches to this problem are Rosenblatt and Optimal.

The \emph{Rosenblatt's perceptron learning algorithm} aims to find the optimal hyperplane in an iterative way. The algorithm minifies
the distance of misclassified points to the decision boundary. The goal of this algorithm is to minimize the following equation
\begin{equation}
    D(\beta, \beta_0) = \sum_{i\in M} -y_i(x_i^T \beta + \beta_0)
\end{equation}
where $M$ is the set of misclassified points and $y_i$ is the class label of $x_i$, being either 1 or -1. Using stochastic gradient
descent, the algorithm updates the coefficients and intercept. 
\begin{equation}
    \partial\frac{D(\beta, \beta_0)}{\partial\beta} = -\sum_{i\in M} y_ix_i
\end{equation}
\begin{equation}
    \partial\frac{D(\beta, \beta_0)}{\partial\beta_0} = -\sum_{i\in M} y_i
\end{equation}
However, the perceptron learning algorithm is not deterministic, since
the result depends on the starting values. Furthermore, the algorithm does not converge if the data is not linearly separable, resulting
in an infinite loop if the case was not detected beforehand. \cite{Elements4}

In 1996, Vapnik proposed the \emph{optimal separating hyperplane} algorithm in order to cope with the problems of the perceptron learning algorithm.
This algorithm is widely known as maximum margin classifier.
The goal of the algorithm is to find a hyperplane that maximize the distance to the closet point from either class. [Vapnik, 1996]
The observation of both classes is denoted as $y_i = 1$ and $y_i = -1$. The separating hyperplane then has the following property:

\begin{equation}
    X^T \beta + \beta_0 \geq 1, \text{if}\ y_i = 1
\end{equation}
\begin{equation}
    X^T \beta + \beta_0 \leq -1, \text{if}\ y_i = -1
\end{equation}

To summarize:
\begin{equation}
    y_i(X^T \beta + \beta_0) \geq 1,\ i = 1, ..., n
\end{equation}

To solve this problem, we construct the following optimization problem \cite{R9}:
\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0, ||\beta||=1}}{\text{maximize}} \quad
        M \\
      & \text{subject to} \\
      & y_i(x_i^T \beta + \beta_0) \geq M,\ i = 1, ..., n
    \end{aligned}
\end{equation}
where $M$ is the margin. Since the margin is $M$ units away from the hyperplane on either side,
the margin is then $2M$ units wide. The constraint $||\beta|| = 1$ can be also left out by replacing
the condition by
\begin{equation}
    \frac{1}{||\beta||}y_i(x_i^T \beta + \beta_0) \geq M
\end{equation}
If we arbitrarily set $M = \frac{1}{||\beta||}$, then the problem can be then reconstructed as \cite{Elements4}:

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0}}{\text{maximize}} \quad
        \frac 1 {||\beta||} \\
      & \text{subject to} \\
      & y_i(x_i^T \beta + \beta_0) \geq 1,\ i = 1, ..., n
    \end{aligned}
\end{equation}

The two constraints of this optimization problem ensures that each observation is on the right side
and has at least a distance of M from the hyperplane. It can be then solved efficiently using
Lagrange functions.
\begin{equation}
    L_P(\beta, \lambda) = \frac 1 {||\beta||} - \sum_{i=1}^N \lambda_i [ y_i (x_i^T \beta + \beta_0) - 1 ]
\end{equation}
The goal of the Lagrange function is to find the local maxima and minima of a function
that is subjected to equation constraints.
Here, $\frac 1 {||\beta||}$ is the objective function, which is the function that is being 
maximized or minimized, with the constraints being
$y_i(x_i^T \beta + \beta_0) - 1$. $\lambda$ is a vector of the weights of all the training points as support vectors
under the condition of $\lambda_i \geq 0,\ i = 1, ..., n$. From this Lagrange function, we can
derive the following two properties \cite{Elements4}:
\begin{itemize}
    \item if $\lambda_i > 0$, then $y_i(x_i^T\beta + \beta_0) - 1 = 0$, which is equivalent to
    $y_i(x_i^T\beta + \beta_0) = 0$. This means that the observation is on the margin.
    \item if $y_i(x_i^T\beta + \beta_0) - 1 > 0$, or $y_i(x_i^T\beta + \beta_0) > 1$, then
    $\lambda_i = 0$. This means that the observation is not on the margin.
  \end{itemize}




However, the maximum margin classifier has zero tolerance for misclassification. As mentioned before,
the soft margin is introduced to be more susceptible to misclassification. With the mathematical knowledge
given above, we can also define the soft margin classifier. The hyperplane
is chosen to correctly separate most of the dataset into two classes with the tolerance of a few errors.
Thus, we introduce a slack variable $\epsilon_i$, which allows individual observations to be on the wrong side 
of the margin.
The optimization problem is then defined as\cite{R9}:

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0, \epsilon}}{\text{maximize}} \quad
        \frac 1 {||\beta||} \\
      & \text{subject to} \\
      & y_i(x_i^T \beta + \beta_0) \geq \frac{1}{||\beta||}(1-\epsilon_i),\ i = 1, ..., n, \\
      & \epsilon_i \geq 0, \ \sum_{i=1}^n \epsilon_i \leq C
    \end{aligned}
\end{equation}

The interpretation of the slack variable helps us to determine whether an observation is on the correct side of the margin.
The $i$th observation is on the correct side of the margin if $\epsilon_i = 0$. Otherwise, 
the $i$th observation is on the wrong side of the margin if $\epsilon_i > 0$ and on the wrong side of the hyperplane
if $\epsilon_i > 1$. The parameter $C$, in this case, is the tuning parameter. It determines the budget of violations
that could happen in this classification problem. The choice of $C$ plays a decisive role in the learning process.
If $C$ is large, then many observations violate the margin, which causes the involvement of many observations in 
determining the hyperplane. On the other hand, smaller $C$s result in a classifier with lower bias but higher variance.
In practice, cross-validation is used to choose the ideal $C$. 

Based on the optimization problem given above, we can derive the formal definition of SVM. In comparison to 
soft margin classifier, the SVM transform the data into a higher dimension space $p'$, so that the data could be
linearly separable. For instance, we transform the features $X_1, X_2, ..., X_p$ into $X_1, X_1^{p'} X_2^{p'}, ..., X_p^{p'}$.
The optimization problem with the transformed data can then be altered to\cite{R9}: 

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta_0, \beta_{11}, \beta_{12}, ..., \beta_{p(p-1)'}, \beta_{pp'}, \epsilon_1, ..., \epsilon_n}}{\text{maximize}} \quad
        \frac 1 {||\beta||}\\
      & \text{subject to} \\
      & y_1(\beta_0 + \sum_{j=1}^p \sum_{k=1}^{p'} \beta_{jk}x_{ij}^k )\geq \frac{1}{||\beta||} (1-\epsilon_i) \\
      & \sum_{i=1}^n\epsilon_i \leq C,\  \epsilon_i \geq 0, \ \sum_{j=1}^p\sum_{k=1}^{p'}\beta_{jk}^2 = 1
    \end{aligned}
\end{equation}

In order to perform the transformation and simplify this problem, we use a kernel function $K(x, x')$. It is defined as \cite{Elements12}:
\begin{equation}
    K(x, x') = \langle h(x), h(x') \rangle
\end{equation}
where $h(x)$ is a transformation function. 
The transformation process leaves us with a lot of freedom, and we are able to freely decide how we should transform the
data. The two popular choices of kernel functions in practical applications are:
\begin{itemize}
    \item dth-Degree Polynomial kernel: $K(x, x') = (1 + \langle x, x' \rangle)^d$
    \item Radial kernel: $K(x, x') = \exp(-\gamma ||x - x'||^2)$
\end{itemize}

First, let us discuss the polynomial kernel. The polynomial kernel transforms each data point into a polynomial
of degree $d$ and fits a support vector classifier into this higher-dimensional space. This 
transformation leads to a flexible decision boundary. 
\begin{equation}
    K(x, x') = (1 + \langle x, x' \rangle)^d = (1 + \sum_{j=1}^px_{ij}x_{i'j})^d
\end{equation}

Another popular choice is the radial kernel. In order to understand the radial kernel, we first need to understand
the following scenario.
If a given test observation $x^*$ is far away from the observation $x_i$,
then the Euclidean distance between the two observations will be large.
As the result, the exponential term of the negative value of the Euclidean distance will be close to zero,
which means that the presence of $x^*$ will not have much effect on the transformation.
On the other hand, if $x^*$ is close to $x_i$, then the exponential term will be close to one, which
will exert a strong influence on the transformation. The radial kernel utilizes this property of the
exponential function to emphasize the points that are close to the test observation $x^*$ \cite{R9}.

\begin{equation}
    K(x, x') = \exp(-\gamma ||x - x'||^2) = \exp(-\gamma \sum_{j=1}^p(x_{ij} - x_{i'j})^2)
\end{equation}

The implementation of kernels in SVM saves computational power, since for each pair of data points, we
only need to calculate the transformed value once. Furthermore, the transformative property of the
kernel leads to a flexible and precise boundary. \cite{R9}
