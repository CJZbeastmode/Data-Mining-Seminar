\section{implementation}
The problem of support vector machine is an optimization problem. The goal is, as mentioned, 
to find the optimal hyperplane that separates the data in a way that the margin is maximized.
Therefore, it is important to first define the hyperplane.

Since the hyperplane is a flat affine subspace of dimension $p$, we can easily define a hyperplane as: (Page 368 R)
\begin{equation}
    \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0
\end{equation}

We can then denote $X$ and $\beta$ as vectors: (Page 418 elements learning)
\begin{equation}
    \{x \in \mathbb{R}^p: X^T \beta + \beta_0 = 0\}
\end{equation}

With hyperplane being defined, we are now able to observe the problem of separating the hyperplane. 
In other words, we need to construct linear decision
boundaries that attempt to separate the data into two or more classes as precisely as possible. 
The main two mechanics to this problem are Rosenblatt and Optimal.

The \emph{Rosenblatt's perceptron learning algorithm} aims to find the optimal hyperplane in an iterative way. The algorithm minizies
the distance of misclassified points to the decision boundary. The goal of this algorithm is to minimize the following equation
\begin{equation}
    D(\beta, \beta_0) = \sum_{i\in M} -y_i(x_i^T \beta + \beta_0)
\end{equation}
where $M$ is the set of misclassified points and $y_i$ is the class label of $x_i$, being either 1 or -1. Using stochastic gradient
descent, the algorithm updates the coefficients and intercept. 
\begin{equation}
    \partial\frac{D(\beta, \beta_0)}{\partial\beta} = -\sum_{i\in M} y_ix_i
\end{equation}
\begin{equation}
    \partial\frac{D(\beta, \beta_0)}{\partial\beta_0} = -\sum_{i\in M} y_i
\end{equation}
However, the perceptron learning algorithm is not deterministic, since
the result depends on the starting values. Furthermore, the algorithm does not converge if the data is not linearly separable, resulting
in an infinite loop if the case was not detected beforehand.

In 1996, Vapnik proposed the \emph{optimal separating hyperplane} algorithm in order to cope with the problems of the perceptron learning algorithm.
This algorithm is widely implemented as maximum margin classifier.
The goal of the algorithm is to find a hyperplane that maximize the distance to the closet point from either class. [Vapnik, 1996]
The observation of both classes is denoted as $y_i = 1$ and $y_i = -1$. The separating hyperplane then has the following property:

\begin{equation}
    X^T \beta + \beta_0 \geq 1, \text{if}\ y_i = 1
\end{equation}
\begin{equation}
    X^T \beta + \beta_0 \leq -1, \text{if}\ y_i = -1
\end{equation}

To summarize:
\begin{equation}
    y_i(X^T \beta + \beta_0) \geq 1,\ i = 1, ..., n
\end{equation}
(Page 370 R)

To solve this problem, we construct the following optimization problem:
\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0}}{\text{maximize}} \quad
        M \\
      & \text{subject to} \\
      & ||\beta|| = 1, \\
      & y_i(x_i^T \beta + \beta_0) \geq M,\ i = 1, ..., n
    \end{aligned}
\end{equation}
where $M$ is the margin. Since the margin is $M$ units away from the hyperplane on either side,
the margin is then $2M$ units wide. The constraint $||\beta|| = 1$ can be also left out by replacing
the condition by
\begin{equation}
    \frac{1}{||\beta||}y_i(x_i^T \beta + \beta_0) \geq M
\end{equation}
We arbitrarily set $M = \frac{1}{||\beta||}$. The problem can be then reconstructed as:

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0}}{\text{maximize}} \quad
        \frac12 ||\beta||^2 \\
      & \text{subject to} \\
      & y_i(x_i^T \beta + \beta_0) \geq 1,\ i = 1, ..., n
    \end{aligned}
\end{equation}
(Page 132 elements)

The two constraints of this optimization problem ensures that each observation is on the right side
and has at least a distance of M from the hyperplane. It can be then solved in an efficient way using
Lagrange functions. The mathematical deduction is beyond the scope of this paper, 
but the result is as follows:
\begin{equation}
    \beta = \sum_{i=1}^n \alpha_i y_i x_i
\end{equation}
\begin{equation}
    0 = \sum_{i=1}^n \alpha_i y_i
\end{equation}
\begin{equation}
    \alpha_i[y_i(x_i^T\beta + \beta_0) - 1] = 0\ \forall i
\end{equation}
where $\alpha$ is a vector of the weights of all the training points as support vectors
under the condition of $\alpha_i \geq 0,\ i = 1, ..., n$. From these three equations, we can
derive the following two properties:
\begin{itemize}
    \item if $\alpha_i > 0$, then $y_i(x_i^T\beta + \beta_0) - 1 = 0$, which is equivalent to
    $y_i(x_i^T\beta + \beta_0) = 0$. This means that the observation is on the margin.
    \item if $y_i(x_i^T\beta + \beta_0) - 1 > 0$, or $y_i(x_i^T\beta + \beta_0) > 1$, then
    $\alpha_i = 0$. This means that the observation is not on the margin.
  \end{itemize}




However, the maximum margin classifier has zero tolerance for misclassification. As mentioned before,
the support vector is designed to be more susceptible to misclassification. With the mathematical knowledge
given above, we can also define the support vector classifer. The hyperplane
is chosen to correctly separate most of the dataset into two classes with the tolerance of a few errors.
Thus we introduce a slack variable $\epsilon_i$, which allows individual observations to be on the wrong side 
of the margin.
The optimization problem is then defined as:

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0, \epsilon, M}}{\text{maximize}} \quad
        M \\
      & \text{subject to} \\
      & ||\beta|| = 1, \\
      & y_i(x_i^T \beta + \beta_0) \geq M(1-\epsilon_i),\ i = 1, ..., n, \\
      & \epsilon_i \geq 0, \ \sum_{i=1}^n \epsilon_i \leq C
    \end{aligned}
\end{equation}

(page 374 R)
The interpretation of the slack variable helps us to determine whether an observation is on the correct side of the margin.
The $i$th observation is on the correct side of the margin if $\epsilon_i = 0$. Otherwise, 
the $i$th observation is on the wrong side of the margin if $\epsilon_i > 0$ and on the wrong side of the hyperplane
if $\epsilon_i > 1$. The parameter $C$, in this case, is the tuning paramter. It determines the budget of violations
that could happen in this classification problem. The choice of $C$ plays a decisive role in the learning process.
If $C$ is large, then many observations violate the margin, which causes the involvement of many observations in 
determining the hyperplane. On the other hand, smaller $C$s result in a classifier with lower bias but higher varianace.
In practice, $C$ is chosen using cross-validation. 

Based on the optimization problem given above, we can derive the formal definition of SVM. In comparison to 
support vector classifier, the SVM transform the data into a higher dimension space $p'$, so that the data could be
linearly separable. For instance, we transform the features $X_1, X_2, ..., X_p$ into $X_1, X_1^{p'} X_2^{p'}, ..., X_p^{p'}$.
The optimization problem with the transformed data can then be altered to: (R 380)

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta_0, \beta_{11}, \beta_{12}, ..., \beta_{p(p-1)'}, \beta_{pp'}, \epsilon_1, ..., \epsilon_n, M}}{\text{maximize}} \quad
        M \\
      & \text{subject to} \\
      & y_1(\beta_0 + \sum_{j=1}^p \sum_{k=1}^{p'} \beta_{jk}x_{ij}^k )\geq M (1-\epsilon_i) \\
      & \sum_{i=1}^n\epsilon_i \leq C,\  \epsilon_i \geq 0, \ \sum_{j=1}^p\sum_{k=1}^{p'}\beta_{jk}^2 = 1
    \end{aligned}
\end{equation}

In order to perform the transformation and simplify this problem, we use a kernel function $K(x, x')$. It is defined as: (elements 424)
\begin{equation}
    K(x, x') = \langle h(x), h(x') \rangle
\end{equation}
where $h(x)$ is a transformation function. 
The transformation process leaves us with a lot of freedom, and we are able to freely decide how we should transform the
data. The two popular choices of kernel functions in pracitcal applications are:
\begin{itemize}
    \item dth-Degree Polynomial kernel: $K(x, x') = (1 + \langle x, x' \rangle)^d$
    \item Radial kernel: $K(x, x') = \exp(-\gamma ||x - x'||^2)$
\end{itemize}

Let us first take a look at the polynomial kernel. The polynomial kernel transforms each data point into a polynomial
of degree $d$ and fits a support vector classifier into this higher-dimensional space. This 
transformation leads to a flexible decision boundary. 
\begin{equation}
    K(x, x') = (1 + \langle x, x' \rangle)^d = (1 + \sum_{j=1}^px_{ij}x_{i'j})^d
\end{equation}

Another popular choice is the radial kernel. In order to understand the radial kernel, we first need to understand
the following scenario.
If a given test observation $x^*$ is far away from the observation $x_i$,
then the Euclidean distance between the two points will be large.
As a result, the exponential term of the negative value of the Euclidean distance will be close to zero.
In this case, the presence of $x^*$ will not have much effect on the transformation.
On the other hand, if $x^*$ is close to $x_i$, then the exponential term will be close to one, which
will exert a strong influence on the transformation. The radial kernel utilizes this property of the
exponential function to emphasize the points that are close to the test observation $x^*$.

\begin{equation}
    K(x, x') = \exp(-\gamma ||x - x'||^2) = \exp(-\gamma \sum_{j=1}^p(x_{ij} - x_{i'j})^2)
\end{equation}

The implementation of kernels in SVM saves computational power, since for each pair of data points, we
only need to calculate the transformed value once. Furthermore, the transformative property of the
kernel leads to a flexible and precise boundary.
