\section{implementation}
The problem of support vector machine is an optimization problem. The goal is, as mentioned, 
to find the optimal hyperplane that separates the data in a way that the margin is maximized.
Therefore, it is important to first define the hyperplane.

Since the hyperplane is a flat affine subspace of dimension $p$, we can easily define a hyperplane as: (Page 368 R)
\begin{equation}
    \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0
\end{equation}

We can then denote $X$ and $\beta$ as vectors: (Page 418 elements learning)
\begin{equation}
    \{x \in \mathbb{R}^p: X^T \beta + \beta_0 = 0\}
\end{equation}

With hyperplane being defined, we are now able to observe the problem of separating the hyperplane. 
In other words, we need to construct linear decision
boundaries that attempt to separate the data into two or more classes as precisely as possible. 
The main two mechanics to this problem are Rosenblatt and Optimal.

The \emph{Rosenblatt's perceptron learning algorithm} aims to find the optimal hyperplane in an iterative way. The algorithm minizies
the distance of misclassified points to the decision boundary. The goal of this algorithm is to minimize the following equation
\begin{equation}
    D(\beta, \beta_0) = \sum_{i\in M} -y_i(x_i^T \beta + \beta_0)
\end{equation}
where $M$ is the set of misclassified points and $y_i$ is the class label of $x_i$, being either 1 or -1. Using stochastic gradient
descent, the algorithm updates the coefficients and intercept. 
\begin{equation}
    \partial\frac{D(\beta, \beta_0)}{\partial\beta} = -\sum_{i\in M} y_ix_i
\end{equation}
\begin{equation}
    \partial\frac{D(\beta, \beta_0)}{\partial\beta_0} = -\sum_{i\in M} y_i
\end{equation}
However, the perceptron learning algorithm is not deterministic, since
the result depends on the starting values. Furthermore, the algorithm does not converge if the data is not linearly separable, resulting
in an infinite loop if the case was not detected beforehand.

In 1996, Vapnik proposed the \emph{optimal separating hyperplane} algorithm in order to cope with the problems of the perceptron learning algorithm.
This algorithm is widely implemented as maximum margin classifier.
The goal of the algorithm is to find a hyperplane that maximize the distance to the closet point from either class. [Vapnik, 1996]
The observation of both classes is denoted as $y_i = 1$ and $y_i = -1$. The separating hyperplane then has the following property:

\begin{equation}
    X^T \beta + \beta_0 \geq 1, \text{if}\ y_i = 1
\end{equation}
\begin{equation}
    X^T \beta + \beta_0 \leq -1, \text{if}\ y_i = -1
\end{equation}

To summarize:
\begin{equation}
    y_i(X^T \beta + \beta_0) \geq 1,\ i = 1, ..., n
\end{equation}
(Page 370 R)

To solve this problem, we construct the following optimization problem:
\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0}}{\text{maximize}} \quad
        M \\
      & \text{subject to} \\
      & ||\beta|| = 1, \\
      & y_i(x_i^T \beta + \beta_0) \geq M,\ i = 1, ..., n
    \end{aligned}
\end{equation}
where $M$ is the margin. Since the margin is $M$ units away from the hyperplane on either side,
the margin is then $2M$ units wide. The constraint $||\beta|| = 1$ can be also left out by replacing
the condition by
\begin{equation}
    \frac{1}{||\beta||}y_i(x_i^T \beta + \beta_0) \geq M
\end{equation}
We arbitrarily set $M = \frac{1}{||\beta||}$. The problem can be then reconstructed as:

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0}}{\text{maximize}} \quad
        \frac12 ||\beta||^2 \\
      & \text{subject to} \\
      & y_i(x_i^T \beta + \beta_0) \geq 1,\ i = 1, ..., n
    \end{aligned}
\end{equation}
(Page 132 elements)

The two constraints of this optimization problem ensures that each observation is on the right side
and has at least a distance of M from the hyperplane. It can be then solved in an efficient way using
Lagrange functions. The mathematical deduction is beyond the scope of this paper, 
but the result is as follows:
\begin{equation}
    \beta = \sum_{i=1}^n \alpha_i y_i x_i
\end{equation}
\begin{equation}
    0 = \sum_{i=1}^n \alpha_i y_i
\end{equation}
\begin{equation}
    \alpha_i[y_i(x_i^T\beta + \beta_0) - 1] = 0\ \forall i
\end{equation}
where $\alpha$ is a vector of the weights of all the training points as support vectors
under the condition of $\alpha_i \geq 0,\ i = 1, ..., n$. From these three equations, we can
derive the following two properties:
\begin{itemize}
    \item if $\alpha_i > 0$, then $y_i(x_i^T\beta + \beta_0) - 1 = 0$, which is equivalent to
    $y_i(x_i^T\beta + \beta_0) = 0$. This means that the observation is on the margin.
    \item if $y_i(x_i^T\beta + \beta_0) - 1 > 0$, or $y_i(x_i^T\beta + \beta_0) > 1$, then
    $\alpha_i = 0$. This means that the observation is not on the margin.
  \end{itemize}




However, the maximum margin classifier has zero tolerance for misclassification. As mentioned before,
the support vector is designed to be more susceptible to misclassification. With the mathematical knowledge
given above, we can also define the support vector classifer. The hyperplane
is chosen to correctly separate most of the dataset into two classes with the tolerance of a few errors.
Thus we introduce a slack variable $\epsilon_i$, which allows individual observations to be on the wrong side 
of the margin.
The optimization problem is then defined as:

\begin{equation}
    \begin{aligned}
      & \underset{\textstyle {\beta, \beta_0, \epsilon, M}}{\text{maximize}} \quad
        M \\
      & \text{subject to} \\
      & ||\beta|| = 1, \\
      & y_i(x_i^T \beta + \beta_0) \geq M(1-\epsilon_i),\ i = 1, ..., n, \\
      & \epsilon_i \geq 0, \ \sum_{i=1}^n \epsilon_i \leq C
    \end{aligned}
\end{equation}

(page 374 R)
The interpretation of the slack variable helps us to determine whether an observation is on the correct side of the margin.
The $i$th observation is on the correct side of the margin if $\epsilon_i = 0$. Otherwise, 
the $i$th observation is on the wrong side of the margin if $\epsilon_i > 0$ and on the wrong side of the hyperplane
if $\epsilon_i > 1$. The parameter $C$, in this case, is the tuning paramter. It determines the budget of violations
that could happen in this classification problem. The choice of C plays a decisive role in the learning process.
If $C$ is large, then many observations violate the margin, which causes the involvement of many observations in 
determining the hyperplane. On the other hand, smaller $C$s result in a classifier with lower bias but higher varianace.
In practice, $C$ is chosen using cross-validation. 

With the knowledge given above, we can formally derive the optimization problem for support vector machine:
\begin{equation}
    \begin{aligned}
    \max_{\beta, \beta_0, \epsilon} \frac12{||\beta||}^2 + C\sum_{i=1}^n \epsilon_i \\
    \text{subject to}\  y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_px_{ip}) \geq 1 - \epsilon_i\  \forall\ i = 1, ..., n\\
    \epsilon_i \geq 0
    \end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\max_{\beta, \beta_0, \epsilon} \quad & \frac12{||\beta||}^2 + C\sum_{i=1}^n \epsilon_i\\
\textrm{subject to} \quad & y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_px_{ip}) \geq 1 - \epsilon_i\ \\
  &\xi\geq0    \\
  \epsilon_i \geq 0 \\
\end{aligned}
\end{equation}


In order to separate the non-linearly separable data, we can 
The kernel function is defined as:
\begin{equation}
    K(x_i, x_j) = \phi(x_i)^T \phi(x_j)
\end{equation}
